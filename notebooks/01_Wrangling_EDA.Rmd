---
title: "R Notebook"
output: html_notebook
author: Marco Polo Bravo Montiel
date: 2020-04-21
---

### Libraries

```{r}
# if (!require("renv")) install.packages("renv")
# library(renv)
# renv::restore()
library(here)
library(dplyr)
library(readr)
library(arrow)
```

### Read data

```{r}
zip_file <- here("data", "raw", "iter_00_cpv2020_csv.zip")
```

```{r}
temp_dir <- here("temp")
dir.create(temp_dir, showWarnings = FALSE)

unzip(zip_file, files = c("iter_00_cpv2020/conjunto_de_datos/conjunto_de_datos_iter_00CSV20.csv", "iter_00_cpv2020/diccionario_datos/diccionario_datos_iter_00CSV20.csv"), exdir = temp_dir)
```

```{r}

data_path <- here(temp_dir,
                 "iter_00_cpv2020",
                 "conjunto_de_datos",
                 "conjunto_de_datos_iter_00CSV20.csv")

dict_path <- here(temp_dir,
                 "iter_00_cpv2020",
                 "diccionario_datos",
                 "diccionario_datos_iter_00CSV20.csv")

info_dict <- read_csv(dict_path)
df <- read_csv(data_path)


unlink(temp_dir, recursive = TRUE)
```

```{r}
# Exporting dictionary file
write_csv(info_dict,
          here("data", "raw", "diccionario_datos_iter_00CSV20.csv"))

```

### Exploration

```{r}
head(df)
head(info_dict)
```

```{r}
str(df)
```

```{r}
clean_info_dict <- info_dict[-c(1:3), ]
names(clean_info_dict) <- clean_info_dict[1, ]
clean_info_dict <- clean_info_dict[-1,]
clean_info_dict <- clean_info_dict[, -c(7:10)]


clean_info_dict
```

### Replacing names for consistency

```{r}
# replacement_rules <- list(
#   "Coahuila de Zaragoza" = "Coahuila",
#   "Michoacán de Ocampo" = "Michoacán",
#   "Veracruz de Ignacio de la Llave" = "Veracruz"
# )
# 
# df_new_names <- df |> 
#     mutate(NOM_ENT = case_when(
#       NOM_ENT %in% names(replacement_rules) ~ replacement_rules[NOM_ENT],
#       TRUE ~ NOM_ENT
#   ))


replacement_dict <- c(
  "Coahuila de Zaragoza" = "Coahuila",
  "Michoacán de Ocampo" = "Michoacán",
  "Veracruz de Ignacio de la Llave" = "Veracruz"
)

df_new_names <- df |> 
        mutate(NOM_ENT = ifelse(NOM_ENT %in% names(replacement_dict),
                          replacement_dict[NOM_ENT],
                          NOM_ENT))
```

```{r}
unique_states <- df_new_names |> 
      distinct(NOM_ENT)

write_csv(unique_states, here("data", "processed", "entity_names.csv"))


unique_states
```

```{r}
entities_csv <- read_csv(here("data", "processed", "entity_names.csv")) |> pull()

print(entities_csv)
```

### Selecting rows that we'll analyze

```{r}
rows_to_include <- c(1:12, 53:132, 136:140, 147, 155:211, 220:232)

filtered_data <- clean_info_dict |> 
      filter(row_number() %in% rows_to_include) |> 
      pull(4)

filtered_data
```

```{r}
selected_df <- df_new_names |> 
      select(filtered_data)

selected_df
```

### EDA before exporting

```{r}
str(df)
```

### Exporting as parquet

```{r}
# Export wrangled data as parquet file
table <- arrow::Table$create(selected_df)

output_dir <- here("data", "processed", "parquet_data")

arrow::write_dataset(table, output_dir, partitioning = c("NOM_ENT", "ENTIDAD"), existing_data_behavior = "overwrite")
```

### Reading parquet

```{r}
ds <- open_dataset(here("data", "processed", "parquet_data")) |> 
        collect()

ds
```

### Puebla

```{r}
ds_puebla <- open_dataset(here("data", "processed", "parquet_data")) |>
    filter(NOM_ENT=="Puebla") |> 
    collect()

ds_puebla
```

### Yucatán

```{r}
ds_yucatan <- open_dataset(here("data", "processed", "parquet_data")) |>
    filter(NOM_ENT=="Yucatán") |> 
    collect()

ds_yucatan
```

### Nuevo León

```{r}
ds_nuevo_leon <- open_dataset(here("data", "processed", "parquet_data")) |>
    filter(NOM_ENT=="Nuevo León") |> 
    collect()

ds_nuevo_leon
```

### Total Nacional

```{r}
ds_nacional <- open_dataset(here("data", "processed", "parquet_data")) |>
    filter(NOM_ENT=="Total nacional") |> 
    collect()

ds_nacional
```

### Verify datasets are not empty

```{r}

for(value in entities_csv) {
  
  read_dfs <- open_dataset(here("data", "processed", "parquet_data")) |>
    filter(NOM_ENT==value) |>
    collect()

  if (nrow(read_dfs) == 0) {
    print(paste("Dataset is empty", value))
  } else {
        print(paste("OK", value, nrow(read_dfs)))

  }

}
```

### Coordinate Lab

```{r}
longitudes <- selected_df$LONGITUD
latitudes <- selected_df$LATITUD
```

```{r}
test_long <- longitudes[8]
test_long
```

```{r}
sections <- unlist(strsplit(test_long, "[°'\" ]"))
degrees <- as.numeric(sections[1])
minutes <- as.numeric(sections[2])
seconds <- as.numeric(sections[3])

decimal_degrees <- (degrees + minutes / 60 + seconds / 3600) * -1
decimal_degrees
```

```{r}
longitude_to_decimal <- function(test_long) {
    if (is.na(test_long)) {
    return(NA)  
    }
  
  sections <- unlist(strsplit(test_long, "[°'\" ]"))
  
  degrees <- as.numeric(sections[1])
  minutes <- as.numeric(sections[2])
  seconds <- as.numeric(sections[3])

  
  decimal_degrees <- (degrees + minutes / 60 + seconds / 3600) * -1
}

latitude_to_decimal <- function(test_lat) {
    if (is.na(test_lat)) {
    return(NA)  
    }
  
  sections <- unlist(strsplit(test_lat, "[°'\" ]"))
  
  degrees <- as.numeric(sections[1])
  minutes <- as.numeric(sections[2])
  seconds <- as.numeric(sections[3])

  
  decimal_degrees <- (degrees + minutes / 60 + seconds / 3600)
}
```

```{r}
selected_clean <- selected_df |> 
      mutate(longitude_decimal = sapply(LONGITUD, longitude_to_decimal),
             latitude_decimal = sapply(LATITUD, latitude_to_decimal))

selected_clean
```

### Exporting clean

```{r}
table <- arrow::Table$create(selected_clean)

output_dir <- here("data", "processed", "parquet_data_coords")

arrow::write_dataset(table, output_dir, partitioning = c("NOM_ENT", "ENTIDAD"), existing_data_behavior = "overwrite")
```

### Verify unique cities per State

```{r}
for(value in entities_csv) {
  
  read_dfs <- open_dataset(here("data", "processed", "parquet_data")) |>
    filter(NOM_ENT==value) |>
    collect()

  if (nrow(read_dfs) == length(unique(read_dfs$NOM_LOC))) {
    print(paste("Localities Unique", value))
  } else {
        print(paste("NOT OK", value, nrow(read_dfs), "<>", length(unique(read_dfs$NOM_LOC))))

  }

}
```

```{r}
for(value in entities_csv) {
  
  read_dfs <- open_dataset(here("data", "processed", "parquet_data")) |>
    filter(NOM_ENT==value) |>
    collect()
  
    read_dfs$NOM_MUN_LOC <- paste(read_dfs$NOM_MUN, read_dfs$NOM_LOC, sep = "_")

  if (nrow(read_dfs) == length(unique(read_dfs$NOM_MUN_LOC))) {
    print(paste("Localities Unique", value))
  } else {
        print(paste("NOT OK", value, nrow(read_dfs), "<>", length(unique(read_dfs$NOM_MUN_LOC))))

  }

}
```

```{r}
for(value in entities_csv) {
  
  read_dfs <- open_dataset(here("data", "processed", "parquet_data")) |>
    filter(NOM_ENT==value) |>
    collect()
  
    read_dfs$NOM_LOC_LOC <- paste(read_dfs$LOC, read_dfs$NOM_LOC, sep = "_")

  if (nrow(read_dfs) == length(unique(read_dfs$NOM_LOC_LOC))) {
    print(paste("Localities Unique", value))
  } else {
        print(paste("NOT OK", value, nrow(read_dfs), "<>", length(unique(read_dfs$NOM_LOC_LOC))))

  }

}
```

```{r}
for(value in entities_csv) {
  
  read_dfs <- open_dataset(here("data", "processed", "parquet_data")) |>
    filter(NOM_ENT==value) |>
    collect()
  
    read_dfs$LOC_MUN <- paste(read_dfs$LOC, read_dfs$MUN, sep = "_")

  if (nrow(read_dfs) == length(unique(read_dfs$LOC_MUN))) {
    print(paste("Localities Unique", value))
  } else {
        print(paste("NOT OK", value, nrow(read_dfs), "<>", length(unique(read_dfs$LOC_MUN))))

  }

}
```

### Conclusion

#### Append code to MUN and LOC
